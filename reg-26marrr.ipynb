{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74a688b3-9fc1-459a-85c4-831cb85e6987",
   "metadata": {},
   "outputs": [],
   "source": [
    "Simple linear regression involves predicting a dependent variable (Y) using only one independent variable (X). \n",
    "It assumes a linear relationship between X and Y. The formula for simple linear regression is:\n",
    "\n",
    "𝑌=𝛽0+𝛽1𝑋+𝜖\n",
    "An example of simple linear regression could be predicting a persons weight (Y) based on their height (X). \n",
    "Here, height is the only independent variable used to predict weight.\n",
    "\n",
    "\n",
    "Multiple linear regression, on the other hand, involves predicting a dependent variable (Y) using two or more independent variables (X1, X2, etc.).\n",
    "It also assumes a linear relationship but with multiple predictors. The formula for multiple linear regression is:\n",
    "\n",
    "𝑌=𝛽0+𝛽1𝑋1+𝛽2𝑋2+...+𝛽𝑛𝑋𝑛+𝜖\n",
    "\n",
    "An example of multiple linear regression could be predicting a house price (Y) based on variables like square footage (X1), number of bedrooms (X2), \n",
    "and number of bathrooms (X3). Here, square footage, number of bedrooms, and number of bathrooms are the independent variables used to predict house price.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58246f48-d77d-4ed0-a0df-fbeb1b427d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "Assumptions of Linear Regression\n",
    "\n",
    "1 Linearity: The relationship between the independent variables and the dependent variable should be linear. This means that changes in the independent variables should result in a constant change in the dependent variable.\n",
    "2 Independence of errors: The errors (residuals) should be independent of each other. There should be no systematic pattern in the residuals.\n",
    "3 Homoscedasticity: The variance of the residuals should be constant across all levels of the independent variables. In other words, the spread of the residuals should be uniform.\n",
    "4 Normality of errors: The residuals should be normally distributed. This assumption ensures that the estimates of the coefficients are unbiased and have minimum variance.\n",
    "5 No multicollinearity: In multiple linear regression, the independent variables should not be highly correlated with each other. Multicollinearity can make it difficult to determine the individual effect of each independent variable on the dependent variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5044adea-1929-4bcd-9074-01856437ae3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "In a linear regression model, the slope and intercept have specific interpretations:\n",
    "\n",
    "Intercept (𝛽0): The intercept represents the value of the dependent variable (Y) when all independent variables (X) are equal to zero. It is the value of Y when X has no effect. In some cases, the intercept may not have a meaningful interpretation, especially if the independent variables cannot realistically be zero.\n",
    "Slope (𝛽1): The slope represents the change in the dependent variable (Y) for a one-unit change in the independent variable (X), holding all other variables constant. It shows the rate of change in Y with respect to X.\n",
    "Here is an example to illustrate the iinterpretation of slope and intercept in a real-world scenario:\n",
    "\n",
    "Let's consider a linear regression model that predicts a student's exam score (Y) based on the number of hours they studied (X). The model equation is:\n",
    "\n",
    "Exam Score=𝛽0+𝛽1×HoursStudied+𝜖\n",
    "\n",
    "\n",
    "Intercept (𝛽0): If the intercept (𝛽0) is 50, it means that a student who studied for zero hours (unlikely in reality but for the sake of explanation) would be expected to score 50 on the exam. This intercept represents the baseline score that a student might achieve without studying.\n",
    "Slope (𝛽1): If the slope (𝛽1) is 5, it means that for each additional hour a student studies, their exam score is expected to increase by 5 points, assuming all other factors remain constant. So, if a student studies for 3 hours (X=3), we would expect their exam score to be\n",
    "50+5×3=65.\n",
    "Interpreting the slope and intercept in this context helps us understand the relationship between hours studied and exam score. The intercept provides a starting point, and the slope indicates the rate of change in exam scores for each additional hour of study.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aeaf3e1-d300-4d20-bf24-d43fe761768f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Gradient descent is an optimization algorithm used to minimize the cost function in machine learning models, especially in training processes like linear regression, logistic regression, neural networks, and more.\n",
    "\n",
    "Here how it works:\n",
    "\n",
    "1 Initialization: It starts by initializing the parameters (coefficients or weights) of the model with some arbitrary values.\n",
    "2 Compute the Gradient: The algorithm calculates the gradient of the cost function with respect to each parameter.\n",
    "The gradient is essentially the direction of steepest ascent. It indicates how much the cost function changes when the parameters are changed.\n",
    "3 Update Parameters: It then updates the parameters in the opposite direction of the gradient to minimize the cost function. \n",
    "This update is performed iteratively and is controlled by a parameter called the learning rate (𝛼)  The learning rate determines the size of the steps taken in the parameter space.\n",
    "4 Convergence: This process continues iteratively until the algorithm converges to a minimum of the cost function or reaches a predefined number of iterations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a14088-1cec-4668-b4e8-7d6a4435036f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Multiple linear regression, on the other hand, involves predicting a dependent variable (Y) using two or more independent variables (X1, X2, etc.).\n",
    "It also assumes a linear relationship but with multiple predictors. The formula for multiple linear regression is:\n",
    "\n",
    "𝑌=𝛽0+𝛽1𝑋1+𝛽2𝑋2+...+𝛽𝑛𝑋𝑛+𝜖\n",
    "\n",
    "An example of multiple linear regression could be predicting a house price (Y) based on variables like square footage (X1), number of bedrooms (X2), \n",
    "and number of bathrooms (X3). Here, square footage, number of bedrooms, and number of bathrooms are the independent variables used to predict house price.\n",
    "\n",
    "The main difference between multiple linear regression and simple linear regression lies in the number of independent variables used to predict the dependent variable. \n",
    "Simple linear regression involves only one independent variable, whereas multiple linear regression involves two or more independent variables.\n",
    "\n",
    "Multiple linear regression allows for the modeling of more complex relationships between the dependent variable and multiple predictors. \n",
    "It enables the analysis of how each independent variable contributes to the variation in the dependent variable while controlling for the effects of other variables.\n",
    "Additionally, multiple linear regression provides more accurate predictions when the relationship between the dependent variable and independent variables is not adequately captured by a single variable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ae7d78-1d65-4f16-a592-87a50a987cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "Multicollinearity refers to the presence of high correlations among independent variables in a multiple linear regression model. It can cause issues in the estimation of coefficients,\n",
    "making them unstable or difficult to interpret. Multicollinearity does not affect the predictive power of the model but can lead to inflated standard errors and incorrect inferences about the individual predictors' effects.\n",
    "\n",
    "Here iss how multicollinearity can occur:\n",
    "\n",
    "1 Perfect multicollinearity: This occurs when one independent variable is a perfect linear function of one or more other variables.\n",
    "For example, if two independent variables are perfectly negatively correlated, it results in perfect multicollinearity.\n",
    "2 High multicollinearity: This occurs when there are high correlations among independent variables but not perfect correlations.\n",
    "High multicollinearity can still lead to issues in the estimation of coefficients.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "004742df-483d-465b-91aa-769944f929d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "Polynomial regression is a type of regression analysis used when the relationship between the independent variable(s) and the dependent variable is nonlinear. While linear regression models the relationship between the dependent variable and independent variable(s) as a linear equation (a straight line), polynomial regression models this relationship as an nth degree polynomial equation.\n",
    "\n",
    "The polynomial regression model can be represented as:\n",
    "\n",
    "𝑌=𝛽0+𝛽1𝑋+𝛽2𝑋2+𝛽3𝑋3+...+𝛽𝑛𝑋𝑛+𝜖\n",
    "\n",
    "The main difference between polynomial regression and linear regression lies in the form of the equation used to model the relationship between the dependent and independent variables. In polynomial regression, the equation is nonlinear due to the inclusion of higher-order terms of the independent variable, while in linear regression, the equation is linear with respect to the parameters.\n",
    "\n",
    "Polynomial regression allows for more flexibility in capturing nonlinear relationships between variables compared to linear regression. It can better fit complex patterns in the data, especially when the relationship between the variables is not adequately captured by a straight line. However, polynomial regression can also lead to overfitting if the degree of the polynomial is too high, resulting in poor generalization to new data.\n",
    "Regularization techniques like ridge regression or model selection methods like cross-validation can help address overfitting in polynomial regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fffca534-ba3f-4dde-9562-1ec17aa47bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "Advantages of Polynomial Regression:\n",
    "\n",
    "1 Flexibility: Polynomial regression can model nonlinear relationships between the independent and dependent variables more effectively than linear regression. It can capture more complex patterns in the data.\n",
    "2 Better Fit: In cases where the relationship between the variables is curved or exhibits peaks and valleys, polynomial regression can provide a better fit to the data compared to linear regression.\n",
    "3 No Assumptions of Linearity: Unlike linear regression, polynomial regression does not assume a linear relationship between the variables. It can accommodate a wider range of relationships without violating assumptions.\n",
    "\n",
    "\n",
    "Disadvantages of Polynomial Regression:\n",
    "\n",
    "1 Overfitting: Polynomial regression can lead to overfitting, especially when using higher-degree polynomials. Overfitting occurs when the model captures noise in the data rather than the underlying pattern, leading to poor generalization to new data.\n",
    "2 Increased Complexity: As the degree of the polynomial increases, the complexity of the model also increases. This can make interpretation more challenging and result in a larger number of parameters to estimate.\n",
    "3 Extrapolation: Extrapolating beyond the range of the observed data with polynomial regression can be risky, as the model may produce unrealistic predictions outside the observed range.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0622b2d-a90e-4e86-936a-a78bef3b7cf3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8708cc9-0b33-4fbd-b05f-352d5dcb4108",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad115015-bc25-433b-b53c-22e4b05091bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c09cc31-7fff-48ac-ac85-8ba3f6fb11dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11ef4224-9878-4093-a237-5c570ecdc700",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
